{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-13.3.1-arm64-arm-64bit\n",
      "Python 3.9.16\n",
      "Pandas 2.0.1\n",
      "Scikit-Learn 1.2.2\n",
      "PyTorch Version: 2.1.0.dev20230507\n",
      "GPU is NOT AVAILABLE\n",
      "MPS (Apple Metal) is AVAILABLE\n",
      "Target device is mps\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "# Get Python and package versions\n",
    "python_version = platform.python_version()\n",
    "pandas_version = pd.__version__\n",
    "sklearn_version = sk.__version__\n",
    "pytorch_version = torch.__version__\n",
    "\n",
    "# Check for GPU and MPS availability\n",
    "has_gpu = torch.cuda.is_available()\n",
    "has_mps = getattr(torch, 'has_mps', False)\n",
    "device = 'mps' if has_mps else 'gpu' if has_gpu else 'cpu'\n",
    "\n",
    "# Print the results\n",
    "print(f'Python Platform: {platform.platform()}')\n",
    "print(f'Python {python_version}')\n",
    "print(f'Pandas {pandas_version}')\n",
    "print(f'Scikit-Learn {sklearn_version}')\n",
    "print(f'PyTorch Version: {pytorch_version}')\n",
    "print(f'GPU is {\"available\" if has_gpu else \"NOT AVAILABLE\"}')\n",
    "print(f'MPS (Apple Metal) is {\"AVAILABLE\" if has_mps else \"NOT AVAILABLE\"}')\n",
    "print(f'Target device is {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-05-10 18:17:32--  http://dlib.net/files/shape_predictor_5_face_landmarks.dat.bz2\n",
      "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
      "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5706710 (5.4M)\n",
      "Saving to: ‘shape_predictor_5_face_landmarks.dat.bz2’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0% 4.66M 1s\n",
      "    50K .......... .......... .......... .......... ..........  1%  925K 4s\n",
      "   100K .......... .......... .......... .......... ..........  2% 1.23M 4s\n",
      "   150K .......... .......... .......... .......... ..........  3% 6.96M 3s\n",
      "   200K .......... .......... .......... .......... ..........  4%  305K 6s\n",
      "   250K .......... .......... .......... .......... ..........  5%  642M 5s\n",
      "   300K .......... .......... .......... .......... ..........  6%  998K 5s\n",
      "   350K .......... .......... .......... .......... ..........  7%  394M 4s\n",
      "   400K .......... .......... .......... .......... ..........  8%  931K 4s\n",
      "   450K .......... .......... .......... .......... ..........  8%  561M 4s\n",
      "   500K .......... .......... .......... .......... ..........  9% 1.08M 4s\n",
      "   550K .......... .......... .......... .......... .......... 10% 17.6M 4s\n",
      "   600K .......... .......... .......... .......... .......... 11% 24.9M 3s\n",
      "   650K .......... .......... .......... .......... .......... 12%  968K 3s\n",
      "   700K .......... .......... .......... .......... .......... 13% 91.4M 3s\n",
      "   750K .......... .......... .......... .......... .......... 14% 1.38M 3s\n",
      "   800K .......... .......... .......... .......... .......... 15% 5.86M 3s\n",
      "   850K .......... .......... .......... .......... .......... 16% 1.68M 3s\n",
      "   900K .......... .......... .......... .......... .......... 17% 3.67M 3s\n",
      "   950K .......... .......... .......... .......... .......... 17%  498K 3s\n",
      "  1000K .......... .......... .......... .......... .......... 18%  669M 3s\n",
      "  1050K .......... .......... .......... .......... .......... 19%  377K 3s\n",
      "  1100K .......... .......... .......... .......... .......... 20%  555M 3s\n",
      "  1150K .......... .......... .......... .......... .......... 21% 15.5M 3s\n",
      "  1200K .......... .......... .......... .......... .......... 22% 1.06M 3s\n",
      "  1250K .......... .......... .......... .......... .......... 23%  509M 3s\n",
      "  1300K .......... .......... .......... .......... .......... 24% 2.25M 3s\n",
      "  1350K .......... .......... .......... .......... .......... 25% 1.60M 3s\n",
      "  1400K .......... .......... .......... .......... .......... 26% 8.01M 3s\n",
      "  1450K .......... .......... .......... .......... .......... 26% 1.41M 3s\n",
      "  1500K .......... .......... .......... .......... .......... 27% 5.50M 2s\n",
      "  1550K .......... .......... .......... .......... .......... 28% 1.44M 2s\n",
      "  1600K .......... .......... .......... .......... .......... 29% 11.9M 2s\n",
      "  1650K .......... .......... .......... .......... .......... 30% 9.54M 2s\n",
      "  1700K .......... .......... .......... .......... .......... 31% 5.28M 2s\n",
      "  1750K .......... .......... .......... .......... .......... 32% 1.12M 2s\n",
      "  1800K .......... .......... .......... .......... .......... 33%  461M 2s\n",
      "  1850K .......... .......... .......... .......... .......... 34% 15.6M 2s\n",
      "  1900K .......... .......... .......... .......... .......... 34% 1.18M 2s\n",
      "  1950K .......... .......... .......... .......... .......... 35% 41.7M 2s\n",
      "  2000K .......... .......... .......... .......... .......... 36% 12.2M 2s\n",
      "  2050K .......... .......... .......... .......... .......... 37%  421M 2s\n",
      "  2100K .......... .......... .......... .......... .......... 38%  854K 2s\n",
      "  2150K .......... .......... .......... .......... .......... 39%  210M 2s\n",
      "  2200K .......... .......... .......... .......... .......... 40%  186M 2s\n",
      "  2250K .......... .......... .......... .......... .......... 41% 24.9M 2s\n",
      "  2300K .......... .......... .......... .......... .......... 42%  305K 2s\n",
      "  2350K .......... .......... .......... .......... .......... 43% 15.0M 2s\n",
      "  2400K .......... .......... .......... .......... .......... 43%  549M 2s\n",
      "  2450K .......... .......... .......... .......... .......... 44%  888M 2s\n",
      "  2500K .......... .......... .......... .......... .......... 45%  892K 2s\n",
      "  2550K .......... .......... .......... .......... .......... 46% 61.6M 2s\n",
      "  2600K .......... .......... .......... .......... .......... 47% 76.7M 2s\n",
      "  2650K .......... .......... .......... .......... .......... 48% 16.2M 1s\n",
      "  2700K .......... .......... .......... .......... .......... 49% 1.15M 1s\n",
      "  2750K .......... .......... .......... .......... .......... 50% 27.5M 1s\n",
      "  2800K .......... .......... .......... .......... .......... 51% 20.6M 1s\n",
      "  2850K .......... .......... .......... .......... .......... 52% 20.0M 1s\n",
      "  2900K .......... .......... .......... .......... .......... 52% 1.20M 1s\n",
      "  2950K .......... .......... .......... .......... .......... 53% 6.95M 1s\n",
      "  3000K .......... .......... .......... .......... .......... 54% 10.8M 1s\n",
      "  3050K .......... .......... .......... .......... .......... 55%  525M 1s\n",
      "  3100K .......... .......... .......... .......... .......... 56%  678M 1s\n",
      "  3150K .......... .......... .......... .......... .......... 57% 1.02M 1s\n",
      "  3200K .......... .......... .......... .......... .......... 58%  503M 1s\n",
      "  3250K .......... .......... .......... .......... .......... 59% 42.2M 1s\n",
      "  3300K .......... .......... .......... .......... .......... 60% 12.9M 1s\n",
      "  3350K .......... .......... .......... .......... .......... 61%  710K 1s\n",
      "  3400K .......... .......... .......... .......... .......... 61%  328M 1s\n",
      "  3450K .......... .......... .......... .......... .......... 62%  588M 1s\n",
      "  3500K .......... .......... .......... .......... .......... 63% 22.4M 1s\n",
      "  3550K .......... .......... .......... .......... .......... 64%  384M 1s\n",
      "  3600K .......... .......... .......... .......... .......... 65%  471K 1s\n",
      "  3650K .......... .......... .......... .......... .......... 66%  359M 1s\n",
      "  3700K .......... .......... .......... .......... .......... 67% 13.2M 1s\n",
      "  3750K .......... .......... .......... .......... .......... 68%  135M 1s\n",
      "  3800K .......... .......... .......... .......... .......... 69% 42.3M 1s\n",
      "  3850K .......... .......... .......... .......... .......... 69%  407K 1s\n",
      "  3900K .......... .......... .......... .......... .......... 70%  407M 1s\n",
      "  3950K .......... .......... .......... .......... .......... 71% 88.0M 1s\n",
      "  4000K .......... .......... .......... .......... .......... 72%  191M 1s\n",
      "  4050K .......... .......... .......... .......... .......... 73% 49.3M 1s\n",
      "  4100K .......... .......... .......... .......... .......... 74% 1.11M 1s\n",
      "  4150K .......... .......... .......... .......... .......... 75% 15.1M 1s\n",
      "  4200K .......... .......... .......... .......... .......... 76% 77.5M 1s\n",
      "  4250K .......... .......... .......... .......... .......... 77% 75.8M 1s\n",
      "  4300K .......... .......... .......... .......... .......... 78% 20.4M 1s\n",
      "  4350K .......... .......... .......... .......... .......... 78% 29.1M 1s\n",
      "  4400K .......... .......... .......... .......... .......... 79% 1.03M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 80%  167M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 81% 77.1M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 82% 50.5M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 83%  775M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 84% 1.09M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 85%  346M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 86% 8.35M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 87%  688M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 87% 22.1M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 88%  414M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 89%  886K 0s\n",
      "  5000K .......... .......... .......... .......... .......... 90% 30.3M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 91%  626M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 92% 40.1M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 93%  669M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 94% 66.4M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 95% 1.12M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 95% 6.94M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 96% 35.1M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 97% 21.1M 0s\n",
      "  5450K .......... .......... .......... .......... .......... 98% 23.1M 0s\n",
      "  5500K .......... .......... .......... .......... .......... 99% 60.1M 0s\n",
      "  5550K .......... .......... ..                              100%  721K=2.2s\n",
      "\n",
      "2023-05-10 18:17:35 (2.53 MB/s) - ‘shape_predictor_5_face_landmarks.dat.bz2’ saved [5706710/5706710]\n",
      "\n",
      "bzip2: Output file shape_predictor_5_face_landmarks.dat already exists.\n",
      "fatal: destination path 'stylegan2-ada-pytorch' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ninja in /Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9/site-packages (1.11.1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# No need to activate conda environment here\n",
    "# conda activate facial_detection_gan_pytorch\n",
    "\n",
    "# Download and extract shape_predictor_5_face_landmarks.dat\n",
    "wget http://dlib.net/files/shape_predictor_5_face_landmarks.dat.bz2\n",
    "bzip2 -d shape_predictor_5_face_landmarks.dat.bz2\n",
    "\n",
    "# Clone StyleGAN2-ada-pytorch repository and install requirements\n",
    "git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
    "pip install ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory:\n",
      "/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks\n",
      "Listing files in the current working directory:\n",
      "jen_1.jpg\n",
      "jen_3.jpeg\n",
      "pytorch_facial_gan_multipleimgs.ipynb\n",
      "pytorch_facial_gan_multipleimgs_v1.py\n",
      "shape_predictor_5_face_landmarks.dat\n",
      "shape_predictor_5_face_landmarks.dat.bz2\n",
      "\u001b[34mstylegan2-ada-pytorch\u001b[m\u001b[m\n",
      "Listing files in the StyleGAN2-ada-pytorch directory:\n",
      "\u001b[31mDockerfile\u001b[m\u001b[m\n",
      "\u001b[31mLICENSE.txt\u001b[m\u001b[m\n",
      "\u001b[31mREADME.md\u001b[m\u001b[m\n",
      "\u001b[31mcalc_metrics.py\u001b[m\u001b[m\n",
      "\u001b[31mdataset_tool.py\u001b[m\u001b[m\n",
      "\u001b[34mdnnlib\u001b[m\u001b[m\n",
      "\u001b[31mdocker_run.sh\u001b[m\u001b[m\n",
      "\u001b[34mdocs\u001b[m\u001b[m\n",
      "\u001b[31mgenerate.py\u001b[m\u001b[m\n",
      "\u001b[31mlegacy.py\u001b[m\u001b[m\n",
      "\u001b[34mmetrics\u001b[m\u001b[m\n",
      "\u001b[31mprojector.py\u001b[m\u001b[m\n",
      "\u001b[31mstyle_mixing.py\u001b[m\u001b[m\n",
      "\u001b[34mtorch_utils\u001b[m\u001b[m\n",
      "\u001b[31mtrain.py\u001b[m\u001b[m\n",
      "\u001b[34mtraining\u001b[m\u001b[m\n",
      "Finding the location of the ninja package:\n",
      "Name: ninja\n",
      "Version: 1.11.1\n",
      "Summary: Ninja is a small build system with a focus on speed\n",
      "Home-page: http://ninja-build.org/\n",
      "Author: Jean-Christophe Fillion-Robin\n",
      "Author-email: scikit-build@googlegroups.com\n",
      "License: Apache 2.0\n",
      "Location: /Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9/site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Current working directory:\"\n",
    "pwd\n",
    "\n",
    "echo \"Listing files in the current working directory:\"\n",
    "ls\n",
    "\n",
    "echo \"Listing files in the StyleGAN2-ada-pytorch directory:\"\n",
    "ls stylegan2-ada-pytorch\n",
    "\n",
    "echo \"Finding the location of the ninja package:\"\n",
    "pip show ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import dlib\n",
    "# from matplotlib import pyplot as plt\n",
    "# import torch\n",
    "# import dnnlib\n",
    "# import legacy\n",
    "# import PIL.Image\n",
    "# import numpy as np\n",
    "# import imageio\n",
    "# from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Set working directory\n",
    "# project_root = \"/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch\"\n",
    "project_root = \"./\"\n",
    "os.chdir(project_root)\n",
    "\n",
    "# Update sys.path\n",
    "sys.path.insert(0, os.path.join(project_root, \"stylegan2-ada-pytorch\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constants\n",
    "NETWORK = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\"\n",
    "STEPS = 150\n",
    "FPS = 30\n",
    "FREEZE_STEPS = 30\n",
    "\n",
    "# Read image files\n",
    "img_list = []\n",
    "for file in os.listdir(project_root):\n",
    "    if file.endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
    "        img_list.append(file)\n",
    "\n",
    "if len(img_list) < 2:\n",
    "    print(\"Upload at least 2 images for morphing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images have been read.\n"
     ]
    }
   ],
   "source": [
    "if len(img_list) > 0:\n",
    "    print(\"The images have been read.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "31ccfa6488004cbbb08e90572f2b9d48",
      "b0811c4b317f462cb9dda2bab1cc397c",
      "52fd07926c3342dab13621046ddea2e1",
      "7d547eace1134adb8451a76beacc6b32",
      "ab4f89000fb64a93a3760400e662444e",
      "0657edff710848db97aefb5f256d5704",
      "7506eb6beb2f4edd9afffb94cf2aa463",
      "e0cd2e01e13946b094a865fcf0c32c92",
      "04d7668efd974730a0c4bb375c8743a1",
      "496009bf1d7445dba1d5256529e5f0af",
      "8348e9ff26cc4e4696cd4d928ec0d824",
      "07d6393119fe4347b4c33f29b72fa524",
      "69ddb3c51515480b8d8ec95098026cf6",
      "3d2ece81a48543d6a19297c0a7a45baa",
      "925a81279bd643b2af5abcb56ce719e3",
      "eb9e883842ee4308bf2a364fae9485ee",
      "0e187e7892ac4220b61446549796983a",
      "bd93a1dc84b942dfb4c256e82faa111b",
      "d31e65ffeb014dc88e2d84bb55177f69",
      "f93c650d4ce145aca2d3e541589180fb",
      "86460ac590c04ec89b4db3b13ba7c2cc",
      "9a9c8fa37f504a4ca3938ec95765b4b8"
     ]
    },
    "id": "dlHshgB1K4-z",
    "outputId": "2f6563d4-ffe3-4029-80c8-99566f2d7374"
   },
   "outputs": [],
   "source": [
    "# # stylegan2 yielded better results than stylegan3 for feature vectors of selfies, so I'll use v2\n",
    "# # 150 imgs for imgs b/w 2 imgs uploaded\n",
    "# # 30 images @ beginning and end since otherwise it's just jumping sequences\n",
    "\n",
    "# NETWORK = \"https://nvlabs-fi-cdn.nvidia.com/\"\\\n",
    "#   \"stylegan2-ada-pytorch/pretrained/ffhq.pkl\"\n",
    "# STEPS = 150\n",
    "# FPS = 30\n",
    "# FREEZE_STEPS = 30\n",
    "\n",
    "# # HIDE OUTPUT\n",
    "# import os\n",
    "# from google.colab import files\n",
    "\n",
    "# uploaded_files = files.upload()\n",
    "\n",
    "# img_list = []\n",
    "# for k, v in uploaded_files.items():\n",
    "#     _, ext = os.path.splitext(k)\n",
    "#     os.remove(k)\n",
    "#     image_name = f\"{k}{ext}\"\n",
    "#     open(image_name, 'wb').write(v)\n",
    "#     img_list.append(image_name)\n",
    "\n",
    "# if len(img_list) < 2:\n",
    "#   print(\"Upload at least 2 images for morphing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current sys.path:\n",
      "['./stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', 'stylegan2-ada-pytorch', './stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python39.zip', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9/lib-dynload', '', '/Users/oscarwu_admin_1.0/.local/lib/python3.9/site-packages', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"Current sys.path:\")\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated sys.path:\n",
      "['stylegan2-ada-pytorch', './stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', 'stylegan2-ada-pytorch', './stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python39.zip', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9/lib-dynload', '', '/Users/oscarwu_admin_1.0/.local/lib/python3.9/site-packages', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9/site-packages']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# HIDE OUTPUT\n",
    "# 5 facial landmark predictor - base of mouth and nose \n",
    "# !wget http://dlib.net/files/shape_predictor_5_face_landmarks.dat.bz2\n",
    "# !bzip2 -d shape_predictor_5_face_landmarks.dat.bz2\n",
    "\n",
    "# # HIDE OUTPUT\n",
    "# import sys\n",
    "# !git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
    "# !pip install ninja\n",
    "# sys.path.insert(0, \"/content/stylegan2-ada-pytorch\")\n",
    "\n",
    "stylegan2_ada_pytorch_dir = os.path.join(\"stylegan2-ada-pytorch\")\n",
    "sys.path.insert(0, stylegan2_ada_pytorch_dir)\n",
    "\n",
    "print(\"Updated sys.path:\")\n",
    "print(sys.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated sys.path:\n",
      "['/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', 'stylegan2-ada-pytorch', './stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', 'stylegan2-ada-pytorch', './stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python39.zip', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9/lib-dynload', '', '/Users/oscarwu_admin_1.0/.local/lib/python3.9/site-packages', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9/site-packages']\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, \"/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch\")\n",
    "\n",
    "print(\"Updated sys.path:\")\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', 'stylegan2-ada-pytorch', './stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', 'stylegan2-ada-pytorch', './stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python39.zip', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9/lib-dynload', '', '/Users/oscarwu_admin_1.0/.local/lib/python3.9/site-packages', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9/site-packages']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', 'stylegan2-ada-pytorch', './stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks/stylegan2-ada-pytorch', 'stylegan2-ada-pytorch', './stylegan2-ada-pytorch', '/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python39.zip', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9/lib-dynload', '', '/Users/oscarwu_admin_1.0/.local/lib/python3.9/site-packages', '/Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_directory = \"/Users/oscarwu_admin_1.0/repos/facial_detection_gan_pytorch/notebooks\"\n",
    "\n",
    "# Insert the stylegan2-ada-pytorch directory\n",
    "sys.path.insert(0, os.path.join(project_directory, \"stylegan2-ada-pytorch\"))\n",
    "\n",
    "# Insert the project_directory\n",
    "sys.path.insert(1, project_directory)\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'stylegan2-ada-pytorch' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "ModuleNotFoundError: No module named 'cv2'\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import cv2; print(cv2.__file__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ninja in /Users/oscarwu_admin_1.0/miniconda3/envs/facial_detection_gan_pytorch/lib/python3.9/site-packages (1.11.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# import cv2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# import numpy as np\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# from PIL import Image\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# import dlib\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# from matplotlib import pyplot as plt\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import dlib\n",
    "# from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import dlib\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import dnnlib\n",
    "import legacy\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import imageio\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_5_face_landmarks.dat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crop_stylegan(img):\n",
    "    dets = detector(img, 1)\n",
    "    if len(dets) == 0:\n",
    "        raise ValueError(\"No face detected\")\n",
    "\n",
    "    d = dets[0]\n",
    "    shape = predictor(img, d)\n",
    "\n",
    "    x1, y1 = shape.part(0).x, shape.part(0).y\n",
    "    x2, y2 = shape.part(2).x, shape.part(2).y\n",
    "    x3, y3 = shape.part(4).x, shape.part(4).y\n",
    "\n",
    "    center = dlib.point((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "    width = np.linalg.norm(np.array([x1, y1]) - np.array([x3, y3]))\n",
    "\n",
    "    size = int(width * 2.2)\n",
    "    half_size = size // 2\n",
    "    left, top = center.x - half_size, center.y - half_size\n",
    "    right, bottom = left + size, top + size\n",
    "\n",
    "    cropped_img = img[top:bottom, left:right]\n",
    "    cropped_img = cv2.resize(cropped_img, (1024, 1024))\n",
    "    return cropped_img\n",
    "\n",
    "def process_images(img_list):\n",
    "    cropped_images = []\n",
    "    for img_name in img_list:\n",
    "        img = cv2.imread(img_name)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"{img_name} not found\")\n",
    "\n",
    "        cropped_img = crop_stylegan(img)\n",
    "        cropped_images.append(cropped_img)\n",
    "\n",
    "        img_rgb = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.title(f'cropped {img_name}')\n",
    "        plt.show()\n",
    "    return cropped_images\n",
    "\n",
    "cropped_images = process_images(img_list)\n",
    "\n",
    "# Generate GAN images and latent vectors for each input image\n",
    "gan_images = []\n",
    "latent_vectors = []\n",
    "\n",
    "for i, cropped_img in enumerate(cropped_images):\n",
    "    cv2.imwrite(f\"cropped_{i}.png\", cropped_img)\n",
    "\n",
    "    # HIDE OUTPUT\n",
    "    cmd = f\"python /content/stylegan2-ada-pytorch/projector.py \"\\\n",
    "      f\"--save-video 0 --num-steps 1000 --outdir=out_{i} \"\\\n",
    "      f\"--target=cropped_{i}.png --network={NETWORK}\"\n",
    "    !{cmd}\n",
    "\n",
    "    img_gan = cv2.imread(f'/content/out_{i}/proj.png')\n",
    "    img_rgb = cv2.cvtColor(img_gan, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(f'gan-image-{i}')\n",
    "    plt.show()\n",
    "\n",
    "    latent_vector = np.load(f'/content/out_{i}/projected_w.npz')['w']\n",
    "    gan_images.append(img_gan)\n",
    "    latent_vectors.append(latent_vector)\n",
    "\n",
    "# Create morph video with all images\n",
    "import torch\n",
    "import dnnlib\n",
    "import legacy\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "import imageio\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "network_pkl = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2\"\\\n",
    "  \"-ada-pytorch/pretrained/ffhq.pkl\"\n",
    "device = torch.device('cuda')\n",
    "with dnnlib.util.open_url(network_pkl) as fp:\n",
    "    G = legacy.load_network_pkl(fp)['G_ema']\\\n",
    "      .requires_grad_(False).to(device)\n",
    "\n",
    "video = imageio.get_writer('/content/movie.mp4', mode='I', fps=FPS, codec='libx264', bitrate='16M')\n",
    "\n",
    "for idx in range(len(latent_vectors) - 1):\n",
    "    lvec1 = latent_vectors[idx]\n",
    "    lvec2 = latent_vectors[idx + 1]\n",
    "\n",
    "    diff = lvec2 - lvec1\n",
    "    step = diff / STEPS\n",
    "    current = lvec1.copy()\n",
    "\n",
    "    for j in tqdm(range(STEPS)):\n",
    "        z = torch.from_numpy(current).to(device)\n",
    "        synth_image = G.synthesis(z, noise_mode='const')\n",
    "        synth_image = (synth_image + 1) * (255/2)\n",
    "        synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255)\\\n",
    "          .to(torch.uint8)[0].cpu().numpy()\n",
    "\n",
    "        repeat = FREEZE_STEPS if j == 0 or j == (STEPS - 1) else 1\n",
    "        for i in range(repeat):\n",
    "            video.append_data(synth_image)\n",
    "        current = current + step\n",
    "\n",
    "video.close()\n",
    "\n",
    "# HIDE OUTPUT\n",
    "from google.colab import files\n",
    "files.download(\"movie.mp4\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "facial_detection_gan_pytorch-3.9",
   "language": "python",
   "name": "facial_detection_gan_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04d7668efd974730a0c4bb375c8743a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0657edff710848db97aefb5f256d5704": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07d6393119fe4347b4c33f29b72fa524": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_69ddb3c51515480b8d8ec95098026cf6",
       "IPY_MODEL_3d2ece81a48543d6a19297c0a7a45baa",
       "IPY_MODEL_925a81279bd643b2af5abcb56ce719e3"
      ],
      "layout": "IPY_MODEL_eb9e883842ee4308bf2a364fae9485ee"
     }
    },
    "0e187e7892ac4220b61446549796983a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31ccfa6488004cbbb08e90572f2b9d48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b0811c4b317f462cb9dda2bab1cc397c",
       "IPY_MODEL_52fd07926c3342dab13621046ddea2e1",
       "IPY_MODEL_7d547eace1134adb8451a76beacc6b32"
      ],
      "layout": "IPY_MODEL_ab4f89000fb64a93a3760400e662444e"
     }
    },
    "3d2ece81a48543d6a19297c0a7a45baa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d31e65ffeb014dc88e2d84bb55177f69",
      "max": 150,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f93c650d4ce145aca2d3e541589180fb",
      "value": 150
     }
    },
    "496009bf1d7445dba1d5256529e5f0af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52fd07926c3342dab13621046ddea2e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0cd2e01e13946b094a865fcf0c32c92",
      "max": 150,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_04d7668efd974730a0c4bb375c8743a1",
      "value": 150
     }
    },
    "69ddb3c51515480b8d8ec95098026cf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e187e7892ac4220b61446549796983a",
      "placeholder": "​",
      "style": "IPY_MODEL_bd93a1dc84b942dfb4c256e82faa111b",
      "value": "100%"
     }
    },
    "7506eb6beb2f4edd9afffb94cf2aa463": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d547eace1134adb8451a76beacc6b32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_496009bf1d7445dba1d5256529e5f0af",
      "placeholder": "​",
      "style": "IPY_MODEL_8348e9ff26cc4e4696cd4d928ec0d824",
      "value": " 150/150 [00:22&lt;00:00,  2.91it/s]"
     }
    },
    "8348e9ff26cc4e4696cd4d928ec0d824": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86460ac590c04ec89b4db3b13ba7c2cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "925a81279bd643b2af5abcb56ce719e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_86460ac590c04ec89b4db3b13ba7c2cc",
      "placeholder": "​",
      "style": "IPY_MODEL_9a9c8fa37f504a4ca3938ec95765b4b8",
      "value": " 150/150 [00:14&lt;00:00, 10.09it/s]"
     }
    },
    "9a9c8fa37f504a4ca3938ec95765b4b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab4f89000fb64a93a3760400e662444e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0811c4b317f462cb9dda2bab1cc397c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0657edff710848db97aefb5f256d5704",
      "placeholder": "​",
      "style": "IPY_MODEL_7506eb6beb2f4edd9afffb94cf2aa463",
      "value": "100%"
     }
    },
    "bd93a1dc84b942dfb4c256e82faa111b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d31e65ffeb014dc88e2d84bb55177f69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0cd2e01e13946b094a865fcf0c32c92": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb9e883842ee4308bf2a364fae9485ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f93c650d4ce145aca2d3e541589180fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
